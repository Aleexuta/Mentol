{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import re \n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from textblob.en import Spelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the URL of the page to scrape\n",
    "url = 'https://zenodo.org/record/3941387#.ZDRcrXZByHv'\n",
    "\n",
    "# set the list of words to match against the file names\n",
    "words_to_match_mental = ['addiction', 'adhd', 'alcoholism'\n",
    "                         'anxiety','autism','bipolarreddit',\n",
    "                         'bpd','depression','EDAnonymous',\n",
    "                         'healthanxiety','lonely','ptsd',\n",
    "                         'schizophrenia','socialanxiety','suicidewatch']\n",
    "words_to_match_nonmental=['conspiracy','divorce','fitness',\n",
    "                         'guns','jokes','legaladvice',\n",
    "                         'meditation','parenting','personalfinance',\n",
    "                         'relationships','teaching']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSV(words):\n",
    "    # send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find all the links that point to CSV files and contain a word from the list\n",
    "    links = soup.find_all('a', class_='filename', href=re.compile(r'\\.csv'))\n",
    "\n",
    "    # download each CSV file\n",
    "    dfs = []\n",
    "    for link in links:\n",
    "        file_url = link['href']\n",
    "        file_name = re.findall(r'[^/]+\\.csv', file_url)[0]\n",
    "        if any(word in file_name for word in words):\n",
    "            final_url='https://zenodo.org' + file_url\n",
    "            response = requests.get(final_url)\n",
    "            print(final_url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                print(f\"Downloaded file {file_name}\")\n",
    "            filtered_df = pd.read_csv(file_name, usecols=['post', 'subreddit'])\n",
    "            #concatenate the dataframes into a single dataframe\n",
    "            dfs.append(filtered_df)\n",
    "            os.remove(file_name)\n",
    "    # concatenate the dataframes into a single dataframe\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    print(\"Combined CSV saved as 'combined.csv'\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://zenodo.org/record/3941387/files/conspiracy_2018_features_tfidf_256.csv?download=1\n",
      "Downloaded file conspiracy_2018_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/conspiracy_2019_features_tfidf_256.csv?download=1\n",
      "Downloaded file conspiracy_2019_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/conspiracy_post_features_tfidf_256.csv?download=1\n",
      "Downloaded file conspiracy_post_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/conspiracy_pre_features_tfidf_256.csv?download=1\n",
      "Downloaded file conspiracy_pre_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/divorce_2018_features_tfidf_256.csv?download=1\n",
      "Downloaded file divorce_2018_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/divorce_2019_features_tfidf_256.csv?download=1\n",
      "Downloaded file divorce_2019_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/divorce_post_features_tfidf_256.csv?download=1\n",
      "Downloaded file divorce_post_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/divorce_pre_features_tfidf_256.csv?download=1\n",
      "Downloaded file divorce_pre_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/fitness_2018_features_tfidf_256.csv?download=1\n",
      "Downloaded file fitness_2018_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/fitness_2019_features_tfidf_256.csv?download=1\n",
      "Downloaded file fitness_2019_features_tfidf_256.csv\n",
      "https://zenodo.org/record/3941387/files/fitness_post_features_tfidf_256.csv?download=1\n",
      "Downloaded file fitness_post_features_tfidf_256.csv\n"
     ]
    }
   ],
   "source": [
    "nonMentalDF=getCSV(words_to_match_nonmental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonMentalDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonMentalDF['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDF['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(nonMentalDF['subreddit'].value_counts()))\n",
    "print(sum(MentalDF['subreddit'].value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDF.to_csv('combined_positive_mental_health.csv', index=False)\n",
    "nonMentalDF.to_csv('combined_negative_mental_health.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDF=pd.read_csv('combined_positive_mental_health.csv')\n",
    "nonMentalDF=pd.read_csv('combined_negative_mental_health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.en import Spelling        \n",
    "import re\n",
    "\n",
    "textToLower = \"\"\n",
    "\n",
    "with open(\"train.txt\",\"r\") as f1:           # Open our source file\n",
    "    text = f1.read()                                  # Read the file                 \n",
    "    textToLower = text.lower()                        # Lower all the capital letters\n",
    "\n",
    "words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
    "oneString = \" \".join(words)                           # Join them into one string\n",
    "\n",
    "pathToFile = \"train.txt\"                              # The path we want to store our stats file at\n",
    "spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
    "spelling.train(oneString, pathToFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lowercase(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "def remove_url(text):\n",
    "    re_url = re.compile('https?://\\S+|www\\.\\S+')\n",
    "    return re_url.sub('', text)\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "def remove_stopwords(text):\n",
    "    new_list = []\n",
    "    words = word_tokenize(text)\n",
    "    stopwrds = stopwords.words('english')\n",
    "    for word in words:\n",
    "        if word not in stopwrds:\n",
    "            new_list.append(word)\n",
    "    return ' '.join(new_list)\n",
    "def steaming(text):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed(full_data):\n",
    "    full_data=full_data.dropna(how='any')\n",
    "    full_data['subreddit'].value_counts().reset_index(name='count')\n",
    "    full_data['post'] = full_data['post'].apply(convert_lowercase)\n",
    "    full_data['post'] = full_data['post'].apply(remove_url)\n",
    "    full_data['post'] = full_data['post'].apply(remove_punc)\n",
    "    full_data['post'] = full_data['post'].apply(remove_stopwords)\n",
    "    full_data['post'] = full_data['post'].apply(lemmatize_words)\n",
    "    full_data['post']=full_data['post'].astype(str).apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    full_data['post'] = full_data['post'].apply(correct_spelling)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDFProcessed=preprocessed(MentalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonMentalDFProcessed=preprocessed(nonMentalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonMentalDFProcessed=nonMentalDFProcessed.drop(['subreddit'],axis=1)\n",
    "nonMentalDFProcessed=['subreddit']='nonmental'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDFProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MentalDFProcessed.to_csv('../datasets/positive_reddit_combined.csv')\n",
    "nonMentalDFProcessed.to_csv('../datasets/negative_reddit_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonMentalDFProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
